## Motivation



Nowadays, it is difficult to find a business that does not collect data about something; humans are 
particular targets of choice for the *Big Data Movement* 
[@web_2016_privacy-international-about-big-data]. Since humans are all individuals, they are 
distinct from each other. While subsets of individuals might share a minor set of attributes, the 
majority is still very unique compared to an individual, given that the overall variety of 
attributes is complex. That small amount of similarity might seem to be of less importance, due to
the nature of inflationary occurrence, but the opposite turns out to be true. 
These similarities allow the determination of whether or not individuals are part of a subset.
Stereotypical patterns are applied to these subsets and thus to all related individuals. This 
enriched information is then used to help predict outcomes of problems or questions related to these 
individuals. In other words, searching for causation where, in best case, one might find 
correlation. This is also known as *discrimination*, which

>   [...] refers to unfair or unequal treatment of people based on membership to a category or a
>   minority, without regard to individual merit 
>   [@paper_2008_discrimination-aware-data-mining, p. 1]. 

Discrimination is a serious issue in our society, caused by humans interacting with each other, 
directly or indirectly, but also when they leverage computers and algorithms to uncover
formerly unnoticed information in order to improve their decision making. For example, when 
qualifying for a loan, hiring employees, investigating crimes or renting flats. The decision to 
approve or deny is based on computed data about the individuals in question
[@book_2015_ethical-it-innovation, chap. 5.6], which is merely discrimination on a much larger scale 
and with less effort (almost parenthetically). 
The described phenomenon is originally referred to as *Bias in computer systems*
[@paper_1996_bias-in-computer-systems]. What at first seems like machines going rouge on humans is, 
in fact, the *cognitive bias* [@wikipedia_2016_cognitive-bias] of human nature, modeled into machine 
executable language and built to reveal the patterns their creators were looking for. The 
*"Inheritance of humanness"* [@web_2016_big-data-is-people, sec. 2], so to say.

In addition to the identity-defining data mentioned before, humans have the habit to create more and 
more data on a daily basis, both pro-actively (e.g by writing a post) and passively (e.g by allowing 
the app to access their current location while submitting the post). As a result, already gigantic 
databases grow ever larger, waiting to be harvested, collected, aggregated, analyzed and finally 
interpreted. The crux here is, the more data being made available 
[@video_2015_big-data-and-deep-learning_discrimination] for mining, the higher the chances of 
isolating datasets (clusters) that differ from each other but are internally coherent. By 
defining those datasets, instead of distinguishing on an individual level, humans are being reduced 
to these set-defining characteristics in order to fit in these clusters.

In order to lower potential discrimination, either the responsible parts in these machines need to
be erased while simultaneously raising awareness and teaching people about this issue of 
discrimination, or all the personal data needs to be prevented from falling into these data silos in 
the first place. Although both approaches are valid and should be pursued simultaneously, the latter 
will be addressed in this work.
